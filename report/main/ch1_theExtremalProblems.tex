\chapter{The two extremal problems}
%\addcontentsline{toc}{chapter}{The two extremal problems}
 \section{The extremal limit problem}
 \subsection{Introduction}
\paragraph{Our approach} We are here interested in proving a result of convergence in distribution $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y$. Anyone who has dabbled in Probability knows that one of the classic ways to prove such a result is to use the definition and show that $\forall x$ in which $F_Y$ is continuous, $F_{Y_n}(x) \xrightarrow[n \rightarrow + \infty]{} F_Y(x)$. Anyone who has dabbled in Statistics knows that in the context of discrete random variables $(Y_n)_{n \ge 0}$, Y it is easier to show the convergence of the probability mass functions i.e. that $\forall x, f_{Y_n}(x) \xrightarrow[n \rightarrow + \infty]{} f_Y(x)$ and conclude by ScheffÃ©'s lemma that $(Y_n)_{n \ge 0}$ converges in distribution to Y. In the context of Extreme Value Theory, though, we will use a result based on the convergence of expectations. It is the \underline{Helly-Bray theorem} : \newline $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y \iff \forall g$ continuous, bounded and real-valued functions, $\mathrm{E}(g(Y_n)) \xrightarrow[n \rightarrow + \infty]{} \mathrm{E}(g(Y))$.
\paragraph{Remark} Our approach here is based upon the notes taken while reading "Chapter 2 : The Probabilistic side of extreme Value Theory" of \underline{Statistics of Extremes, Theory and Applications} by Beirlant \textit{et alii}. \textbf{We thus want to underline that the approach here is not our own, and that we are not engaging in plagiarism but giving account of the work done on our bibliographic sources.} 
\subsection{Solving the problem - step 1}
\paragraph{The answer to the extremal limit problem} It turns out that all possible non-degenerate limiting distributions i.e. all extreme values distribution make up a one-parameter family $G_\gamma(x) = \exp(-(1 + \gamma x)^{-\frac{1}{\gamma}})$, where the support of G is the set $\{ x : 1 + \gamma x > 0}$ and $\gamma \in \mathbb{R}$ is the \textbf{E}xtreme \textbf{V}alue \textbf{I}nded or \textbf{EVI}. This is what we set out to show here.
\paragraph{In the context of our problem} $Y_n = M_n^{*} = \frac{M_n - b_n}{a_n}$ and $Y = Y_\gamma$. $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y \iff \forall g$ continuous, bounded and real-valued functions, $\mathrm{E}(g(Y_n)) \xrightarrow[n \rightarrow + \infty]{} \int_{-\infty}^{+\infty} \! g(v) \, \mathrm{d}G_\gamma(v) $. \newline
\begin{equation}
\begin{alignat*}{2}
\Pr(\{M_n^* \le x\}) &= F_X(x)^n\\ 
\implies \mathrm{E}(g(\frac{M_n - b_n}{a_n})) &= n \int_{-\infty}^{+\infty} \! g(\frac{x - b_n}{a_n}) F_X(x)^{n - 1}\, \mathrm{d}F_X(x)  \\ 
\end{alignat*}
\end{equation} \newline
$F_X : \mathbb{R} \rightarrow [0,1]$, then if $F_X$ is assumed to be continuous\footnote{An assumption that will be satisfied in all what follows}, by the Intermediate Value Theorem there exists in particular a solution to the equation $F(x) = 1 - \frac{v}{n}$. This is equivalent to the equation $x = U(\frac{n}{v}) = Q(1 - \frac{v}{n})$ where $U$ is the tail quantile function of $F_X$ and $Q$ is the quantile function of $F_X$. \newline
\begin{equation}
\begin{alignat*}{2}
F_X(x) &= Q(1 - \frac{v}{n})\\
\implies \mathrm{d}F_X(x) &= -\frac{1}{n} \mathrm{d}v  \\ 
\implies n \int_{-\infty}^{+\infty} \! g(\frac{x - b_n}{a_n}) F_X(x)^{n - 1}\, \mathrm{d}F_X(x) &= n \int_n^0 \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1} (-\frac{1}{n})\, \mathrm{d}v \\
\implies \mathrm{E}(g(\frac{M_n - b_n}{a_n})) &= \int_0^n \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v
\end{alignat*}
\end{equation}\newline where $x = U(\frac{n}{v}) = Q(1 - \frac{v}{n})$ goes from $-\infty$ to $+\infty$. Indeed, \newline
\begin{itemize}
	\item for $v = n$, $x = U(1) = Q(0) = -\infty$
	\item for $v = 0^+$, $x = U(+\infty) = Q(1) = +\infty$
\end{itemize}\newline which explains the change of bounds in the integral in the third equation. We are now working on $\int_0^n \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v$. If we take the limit $n \rightarrow +\infty$, in particular :
\begin{itemize}
	\item the integral will be taken between $0$ and $+\infty$
	\item $(1 - \frac{v}{n})^{n - 1} \xrightarrow[n \rightarrow + \infty]{} (\exp(v))^{-1} = exp(-v)$
\end{itemize}\newline A limit can be obtained for $\mathrm{E}(g(\frac{M_n - b_n}{a_n}))$ when for some sequence of positive numbers $(a_n)_{n \ge 0}$, $\frac{U(\frac{n}{v}) - b_n}{a_n}$ is convergent $\forall v \ge 0$\footnote{Beirlant underlines that if we take $v = 1$, we get the idea that taking $(b_n)_{n \ge 0} = (U(n))$ will guarantee it works.}
\paragraph{A preparatory phase} A condition that must be imposed is the condition $(\mathcal{C})$ : "For some positive function $a$ and $\forall u > 0$, $\frac{U(x u) - U(x)}{a(x)} \xrightarrow[x \rightarrow + \infty]{} h(u)$ where h is not identically equal to 0". \newline
\underline{Proposition :} The possible limits in $(\mathcal{C})$ are given by $c h_\gamma(u) = c \int_1^u \! v^{\gamma -1}\, \mathrm{d}v = c \frac{u^\gamma -1}{\gamma}$, where $c > 0$, $\gamma \ge 0$ and $h_0(u)$ is interpreted as $\log(u)$. \newline \textbf{The case c > 0 can be reduced to $c = 1$ by incorporating $c$ into $a$.}\newline $\forall u$, $v > 0$, \newline
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u v) - U(x)}{a(x)} = \frac{U(x u v) - U(x u)}{a(x u)} \frac{a(x u)}{a(x)} + \frac{U(x u) - U(x)}{a(x)}
\end{alignat*}
\end{equation}\newline If $(\mathcal{C})$ holds then $\frac{a(x u)}{a(x)}$ converges to a $g(u)$.\newline $\forall u$, $v > 0$,
\begin{equation}
\begin{alignat*}{2}
\frac{a(x u v)}{a(x)} &= \frac{a(x u v)}{a(x v)} \frac{a(x v)}{a(x)}\\
\implies \frac{a(x u v)}{a(x)} &= \frac{a((x v) u )}{a(x v)} \frac{a(x v)}{a(x)}\\
\xrightarrow[x \rightarrow + \infty]{} g(u v)  &= g(u) g(v)
\end{alignat*}
\end{equation}\newline We recognize the Cauchy Functional Equation, if a function $g$ satisfies this equation, then $g : \mathbb{R}_+ \longrightarrow \mathbb{R}$ is of the form $u^\gamma$, where $\gamma$ is a real number. Now, if we write $a(x) = x^\gamma l(x)$, \newline
\begin{equation}
\begin{alignat*}{2}
\frac{a(x u v}{a(x)} &= \frac{(x u)^\gamma}{x^\gamma} \frac{l(x u)}{l(x)} \\
&= u^\gamma \frac{l(x u)}{l(x)}
\end{alignat*}
\end{equation} \newline For this quantity to converge to $u^\gamma$, $\frac{l(x u)}{l(x)}$ must converge to . $u$ being a positive, this is equivalent to the fact that $l$ must be a \textit{slowly varying} function. Hence, $a(x) = x^\gamma l(x)$ is a \textit{regularly varying} function with index of regular variation $\gamma$.\newline
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u v) - U(x)}{a(x)} &= \frac{U(x u v) - U(x u)}{a(x u)} \frac{a(x u)}{a(x)} + \frac{U(x u) - U(x)}{a(x)} \\
\xrightarrow[x \rightarrow + \infty]{} h_\gamma(u v) &= h_\gamma(v) u^\gamma + h_\gamma(u)
\end{alignat*}
\end{equation}
\begin{itemize}
	\item \underline{If $\gamma = 0$} \\
	\begin{equation}
	\begin{alignat*}{2}
	h_0(u v) &= h_0(u) + h_0(v) \\
	\implies \exists c \in \mathbb{R} : h_0(u) &= c \log(u)
	\end{alignat*}
	\end{equation}\newline
	\item \underline{If $\gamma \ne 0$}
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u v) &= h_\gamma(v) u^\gamma + h_\gamma(u)
		\end{alignat*}
		\end{equation} And by symmetry,
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u v) &= h_\gamma(u) v^\gamma + h_\gamma(v)
		\end{alignat*}
		\end{equation} Hence,
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u) v^\gamma + h_\gamma(v) &= h_\gamma(v) u^\gamma + h_\gamma(u) \\
		\iff h_\gamma(u) (v^\gamma - 1) &= h_\gamma(v) (u^\gamma - 1) \\
		\implies \exists d : h_\gamma(u) &= d (u^\gamma - 1)
		\end{alignat*}
		\end{equation} where $d$ is a constant, if we take $d = \frac{1}{c \gamma}$, we get $c h_\gamma(u) = \frac{u^\gamma - 1}{\gamma}$
\end{itemize}
To conclude,
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u) - U(x)}{a(x)} \xrightarrow[x \rightarrow + \infty]{} h(u)
\implies h(u) = c h_\gamma(u)
\end{alignat*}
\end{equation} for some constant $c$, with the auxiliary function $a$ regularly varying with index $\gamma$.
\paragraph{Back to the search of an explicit form for the limiting distributions} Let us assume that the condition $(\mathcal{C})$ holds, with $b_n = U(n)$ and $a_n = a(n)$.
\begin{equation}
\begin{alignat*}{2}
\mathrm{E}(g(\frac{M_n - b_n}{a_n})) &= \int_0^n \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v \\
&= \int_0^n \! g(\frac{U(n \frac{1}{v}) - U(n)}{a(n)}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v \\
&\xrightarrow[x \rightarrow + \infty]{} \int_0^{+ \infty} \! g(h_\gamma(\frac{1}{v})) \exp(- v)\, \mathrm{d}v
\end{alignat*}
\end{equation}The last integral must be equal to $\int_{- \infty}^{+ \infty} \! g(u) \, \mathrm{d}G_\gamma(u)$

\subsection{Solving the problem - step 2 : expliciting the $G_\gamma$ in earnest}
\paragraph{Case $\gamma = 0$} We make the following change of variables : $u = h_0(\frac{1}{v}) = \log(\frac{1}{v}) = - \log(v)$. \newline
$u = \log(v) \iff v = \exp(-u)$, with $u \in \mathbb{R}$, $v \in ]0,+ \infty[$.
\begin{equation}
\begin{alignat*}{2}
\mathrm{d}u &= - \frac{1}{v} \mathrm{d}v \\
\implies \int_0^{+ \infty} \! g(u) \exp(- v)\, \mathrm{d}v &= \int_0^{+ \infty} \! g(u) \exp(- \exp(- u)) (- v)\, \mathrm{d}u \\
\implies \int_0^{+ \infty} \! g(u) \exp(- v)\, \mathrm{d}v &= \int_0^{+ \infty} \! g(u) \exp(- \exp(- u)) (- \exp(- u))\, \mathrm{d}u \\
\implies \int_0^{+ \infty} \! g(u) \exp(- v)\, \mathrm{d}v &= \int_{- \infty}^{+ \infty} \! g(u) ( - \exp(- u)\exp(- \exp(- u))) \, \mathrm{d}u \\
\implies \int_0^{+ \infty} \! g(u) \exp(- v)\, \mathrm{d}v &= \int_{- \infty}^{+ \infty} \! g(u) \, \mathrm{d}(\exp(- \exp(- u))) \\
\end{alignat*}
\end{equation}
Finally, \underline{$\gamma > 0 \implies G_\gamma(u) = \exp(- \exp(- u)) $}
\paragraph{Case $\gamma \ne 0$} We make the following change of variables :
\begin{equation}
\begin{alignat*}{2}
u &= h_\gamma(\frac{1}{v}) = \frac{(\frac{1}{v})^\gamma - 1}{\gamma}\\
\iff (\frac{1}{v})^\gamma &= \gamma u + 1\\
\iff \frac{1}{v} &= (\gamma u + 1)^{\frac{1}{\gamma}} \\
\iff v &= (\gamma u + 1)^{- \frac{1}{\gamma}} \\ \\
\mathrm{d}v &= - \frac{1}{\gamma} \gamma (\gamma u + 1)^{- \frac{1}{\gamma} - 1} mathrm{d}u \\
\implies \mathrm{d}v &= - (\gamma u + 1)^{- \frac{\gamma + 1}{\gamma}} \mathrm{d}u
\end{alignat*}
\end{equation} $v$ varies from $0^+$ to $+ \infty$,
\begin{itemize}
	\item \underline{If $\gamma > 0$}, $u = \frac{(\frac{1}{v})^\gamma - 1}{\gamma}$, varying from $- \infty$ to $- \gamma^{-1}$.
	\item \underline{If $\gamma < 0$}, $u = - \frac{v^{- \gamma} + 1}{- \gamma}$, varying from $- \gamma^{-1}$ to $- \infty$.
\end{itemize}
\begin{equation}
\begin{alignat*}{2}
 \int_0^{+ \infty} \! g(u) \exp(- v)\, \mathrm{d}v &=  \int_{+ \infty / - \gamma^{-1}}^{- \gamma^{-1} / - \infty} \! g(u) \exp(- (1 + \gamma u)^{- \frac{1}{\gamma}}) (- (1 + \gamma u)^{- \frac{\gamma + 1}{\gamma}})\, \mathrm{d}u \\
 &=  \int_{ - \gamma^{-1} / - \infty}^{+ \infty / - \gamma^{-1}} \! g(u) (1 + \gamma u)^{- \frac{\gamma + 1}{\gamma}} \exp(- (1 + \gamma u)^{- \frac{1}{\gamma}})  \, \mathrm{d}u \\
\end{alignat*}
\end{equation}
\begin{equation}
\begin{alignat*}{2}
\mathrm{d}(\exp(- (1 + \gamma u)^{- \frac{1}{\gamma}})) &=  (1 + \gamma u)^{- \frac{\gamma + 1}{\gamma}} \exp(- (1 + \gamma u)^{- \frac{1}{\gamma}}) \mathrm{d}u
\end{alignat*}
\end{equation}
Finally, the last integral in $(2.15)$ is equal to,
\begin{itemize}
	\item \underline{If $\gamma > 0$},  $\int_{ - \gamma^{-1}}^{+ \infty} \! g(u) \, \mathrm{d}(\exp(- (1 + \gamma u)^{- \frac{1}{\gamma}}))$ 
	\item \underline{If $\gamma < 0$},  $\int_{ - \infty}^{ - \gamma^{-1}} \! g(u) \, \mathrm{d}(\exp(- (1 + \gamma u)^{- \frac{1}{\gamma}}))$ 
\end{itemize}
Finally, to sum it up :
\begin{itemize}
	\item \underline{$\gamma = 0$ :} \textbf{Gumbel distribution} \newline
	$G_\gamma(u) = \exp(- \exp(- u))$, $u \in \mathbb{R}$
	\item \underline{$\gamma > 0$ :} \textbf{FrÃ©chet distribution} \newline
	$G_\gamma(u) = \exp(- (1 + \gamma u)^{- \frac{1}{\gamma}})$, $u \in ]- \gamma^{-1}, + \infty[$
	\item \underline{$\gamma > 0$ :} \textbf{Weibull distribution} \newline
	$G_\gamma(u) = \exp(- (1 + \gamma u)^{- \frac{1}{\gamma}})$, $u \in ]- \infty, - \gamma^{-1}[$
\end{itemize}
\section{The domain of attraction problem}
\paragraph{Definition} The domain of attraction of an extreme value distribution family (i.e. Gumbel, FrÃ©chet-type or Weibull-type) is the set of distribution functions $F_X$\footnote{$F_X$ being the distribution of the $X_i$ of the sample.} such that the sequence of standardized maxima $(M_n^*)_{n \ge 0}$ will converge in distribution to that extreme value distribution family.
\paragraph{Remark} There are many approaches to characterize the domains of attraction of the extreme value distribution families. We have decided to use Von Mises' theorem to characterize them. This is the historical approach, and a rather straightforward one, still by no means are alternative approaches uninteresting\footnote{In particular, conditions based on the sole behaviour of $F_X$ can be formulated.}.
\paragraph{Hazard function} Let $X$ be a random variable with probability density function/mass function $f_X$ and distribution function $F_X$, then we define the hazard function $r$ as follows : \newline
$r(x) = \frac{f_X(x)}{1 - F_X(x)}$.
\paragraph{A few preliminary notations} $\Phi_\alpha$, $\Psi_\alpha$, $\Delta$ are respectively the symbols used to denote a FrÃ©chet, a Weibull and a Gumbel distributions, with :
\begin{itemize}
	\item $\Phi_\alpha(x) = \exp(- x^\alpha)$
	\item $\Psi_\alpha(x) = \exp(-  \lvert x \rvert^\alpha)$ (let us bear in mind that this is a notation, due to historical reasons).
	\item $\Delta(x) = \exp(- \exp(- x))$
\end{itemize}
\paragraph{Von Mises' theorem}
\begin{enumerate}
	\item If $x^+ = + \infty$ and $x r(x) \xrightarrow[x \rightarrow + \infty]{} \alpha > 0$, then $F_X \in \mathcal{D}(\Phi_\alpha)$.
	\item If $x^+ < + \infty$ and $(x^+ - x) r(x) \xrightarrow[x \rightarrow x^+]{} \alpha > 0$, then $F_X \in \mathcal{D}(\Psi_\alpha)$.
	\item If $r(x)$ is ultimately positive in the neighbourhood of $x^+$, is differentiable on that neighbourhood and is such that $\frac{\mathrm{d}r}{\mathrm{d}x}(x) \xrightarrow[x \rightarrow x^+]{} 0$, then $F_X \in \mathcal{D}(\Delta)$.
\end{enumerate}
\section{Conclusion}
\paragraph{Fisher-Tippett-Gnedenko theorem} The \tetxbf{Fisher-Tippett-Gnedenko} theorem, also known as the \textbf{extremal theorem}, states that if the sequence of standardized maxima converges in distribution to a non-degenerate distribution, then this distribution belongs to one of the three aforementioned extreme value distribution families. The theorem thus provides an answer to the \textit{extremal limit problem}. Von Mises' theorem, encountered in the previous section, provides a complementary answer, that to the \textit{domain of attraction problem}.






