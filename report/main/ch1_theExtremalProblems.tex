\chapter{The two extremal problems}
\addcontentsline{toc}{chapter}{The two extremal problems}
 \section{The extremal limit problem}
 \subsection{Introduction}
\paragraph{Our approach} We are here interested in proving a result of convergence in distribution $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y$. Anyone who has dabbled in Probability knows that one of the classic ways to prove such a result is to use the definition and show that $\forall x$ in which $F_Y$ is continuous, $F_{Y_n}(x) \xrightarrow[n \rightarrow + \infty]{} F_Y(x)$. Anyone who has dabbled in Statistics knows that in the context of discrete random variables $(Y_n)_{n \ge 0}$, Y it is easier to show the convergence of the probability mass functions i.e. that $\forall x, f_{Y_n}(x) \xrightarrow[n \rightarrow + \infty]{} f_Y(x)$ and conclude by ScheffÃ©'s lemma that $(Y_n)_{n \ge 0}$ converges in distribution to Y. In the context of Extreme Value Theory, though, we will use a result based on the convergence of expectations. It is the \underline{Helly-Bray theorem} : \newline $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y \iff \forall g$ continuous, bounded and real-valued functions, $\mathrm{E}(g(Y_n)) \xrightarrow[n \rightarrow + \infty]{} \mathrm{E}(g(Y))$.
\paragraph{Remark} Our approach here is based upon the notes taken while reading "Chapter 2 : The Probabilistic side of extreme Value Theory" of \underline{Statistics of Extremes, Theory and Applications} by Beirlant \textit{et alii}. \textbf{We thus want to underline that the approach here is not our own, and that we are not engaging in plagiarism but giving account of the work done on our bibliographic sources.} 
\subsection{Solving the problem}
\paragraph{The answer to the extremal limit problem} It turns out that all possible non-degenerate limiting distributions i.e. all extreme values distribution make up a one-parameter family $G_\gamma(x) = \exp(-(1 + \gamma x)^{-\frac{1}{\gamma}})$, where the support of G is the set $\{ x : 1 + \gamma x > 0}$ and $\gamma in \mathbb{R}$ is the \textbf{E}xtreme \textbf{V}alue \textbf{I}nded or \textbf{EVI}. This is what we set out to show here.
\paragraph{In the context of our problem} $Y_n = M_n^{*} = \frac{M_n - b_n}{a_n}$ and $Y = Y_\gamma$. $Y_n \xrightarrow[n \rightarrow + \infty]{d} Y \iff \forall g$ continuous, bounded and real-valued functions, $\mathrm{E}(g(Y_n)) \xrightarrow[n \rightarrow + \infty]{} \int_{-\infty}^{+\infty} \! g(v) \, \mathrm{d}G_\gamma(v) $. \newline
\begin{equation}
\begin{alignat*}{2}
\Pr(\{M_n^* \le x\}) &= F_X(x)^n\\ 
\implies \mathrm{E}(g(\frac{M_n - b_n}{a_n})) &= n \int_{-\infty}^{+\infty} \! g(\frac{x - b_n}{a_n}) F_X(x)^{n - 1}\, \mathrm{d}F_X(x)  \\ 
\end{alignat*}
\end{equation} \newline
$F_X : \mathbb{R} \rightarrow [0,1]$, then if $F_X$ is assumed to be continuous\footnote{An assumption that will be assumed to be satisfied in all what follows}, by the Intermediate Value Theorem there exists in particular a solution to the equation $F(x) = 1 - \frac{v}{n}$. This is equivalent to the equation $x = U(\frac{n}{v}) = Q(1 - \frac{v}{n})$ where $U$ is the tail quantile function of $F_X$ and $Q$ is the quantile function of $F_X$. \newline
\begin{equation}
\begin{alignat*}{2}
F_X(x) &= Q(1 - \frac{v}{n})\\
\implies \mathrm{d}F_X(x) &= -\frac{1}{n} \mathrm{d}v  \\ 
\implies n \int_{-\infty}^{+\infty} \! g(\frac{x - b_n}{a_n}) F_X(x)^{n - 1}\, \mathrm{d}F_X(x) &= n \int_n^0 \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1} (-\frac{1}{n})\, \mathrm{d}v \\
\implies \mathrm{E}(g(\frac{M_n - b_n}{a_n})) &= \int_0^n \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v
\end{alignat*}
\end{equation}\newline where $x = U(\frac{n}{v}) = Q(1 - \frac{v}{n})$ goes from $-\infty$ to $+\infty$. Hence, \newline
\begin{itemize}
	\item for $v = n$, $x = U(1) = Q(0) = -\infty$
	\item for $v = 0^+$, $x = U(+\infty) = Q(1) = +\infty$
\end{itemize}\newline which explains the change of bounds in the integral in the third equation. We are now working on $\int_0^n \! g(\frac{U(\frac{n}{v}) - b_n}{a_n}) (1 - \frac{v}{n})^{n - 1}\, \mathrm{d}v$. If we take the limit $n \rightarrow +\infty$, in particular :
\begin{itemize}
	\item the integral will be taken between $0$ and $+\infty$
	\item $(1 - \frac{v}{n})^{n - 1} \xrightarrow[n \rightarrow + \infty]{} (\exp(v))^{-1} = exp(-v)$
\end{itemize}\newline A limit can be obtained for $\mathrm{E}(g(\frac{M_n - b_n}{a_n}))$ when for some sequence of positive numbers $(a_n)_{n \ge 0}$, $\frac{U(\frac{n}{v}) - b_n}{a_n}$ is convergent $\forall v \ge 0$\footnote{Beirlant underlines that if we take $v = 1$, we get the idea that taking $(b_n)_{n \ge 0} = (U(n))$ will guarantee it works.}
\paragraph{A preparatory phase} A condition that must be imposed is the condition $(\mathcal{C})$ : "For some positive function $a$ and $\forall u > 0$, $\frac{U(x u) - U(x)}{a(x)} \xrightarrow[x \rightarrow + \infty]{} h(u)$ where h is not identically equal to 0". \newline
\underline{Proposition :} The possible limits in $(\mathcal{C})$ are given by $c h_\gamma(u) = c \int_1^u \! v^{\gamma -1}\, \mathrm{d}v = c \frac{u^\gamma -1}{\gamma}$, where $c > 0$, $\gamma \ge 0$ and $h_0(u)$ is interpreted as $\log(u)$. \newline \textbf{The case c > 0 can be reduced to $c = 1$ by incorporating $c$ into $a$.}\newline $\forall u$, $v > 0$, \newline
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u v) - U(x)}{a(x)} = \frac{U(x u v) - U(x u)}{a(x u)} \frac{a(x u)}{a(x)} + \frac{U(x u) - U(x)}{a(x)}
\end{alignat*}
\end{equation}\newline If $(\mathcal{C})$ holds then $\frac{a(x u)}{a(x)}$ converges to a $g(u)$.\newline $\forall u$, $v > 0$,
\begin{equation}
\begin{alignat*}{2}
\frac{a(x u v)}{a(x)} &= \frac{a(x u v)}{a(x v)} \frac{a(x v)}{a(x)}\\
\implies \frac{a(x u v)}{a(x)} &= \frac{a((x v) u )}{a(x v)} \frac{a(x v)}{a(x)}\\
\xrightarrow[x \rightarrow + \infty]{} g(u v)  &= g(u) g(v)
\end{alignat*}
\end{equation}\newline We recognize the Cauchy Functional Equation, if a function $g$ satisfies this equation, then $g : \mathbb{R}_+ \longrightarrow \mathbb{R}$ is of the form $u^\gamma$, where $\gamma$ is a real number. Now, if we write $a(x) = x^\gamma l(x)$, \newline
\begin{equation}
\begin{alignat*}{2}
\frac{a(x u v}{a(x)} &= \frac{(x u)^\gamma}{x^\gamma} \frac{l(x u)}{l(x)} \\
&= u^\gamma \frac{l(x u)}{l(x)}
\end{alignat*}
\end{equation} \newline For this quantity to converge to $u^\gamma$, $\frac{l(x u)}{l(x)}$ must converge to . $u$ being a positive, this is equivalent to the fact that $l$ must be a \textit{slowly varying} function. Hence, $a(x) = x^\gamma l(x)$ is a \textit{regularly varying} function with index of regular variation $\gamma$.\newline
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u v) - U(x)}{a(x)} &= \frac{U(x u v) - U(x u)}{a(x u)} \frac{a(x u)}{a(x)} + \frac{U(x u) - U(x)}{a(x)} \\
\xrightarrow[x \rightarrow + \infty]{} h_\gamma(u v) &= h_\gamma(v) u^\gamma + h_\gamma(u)
\end{alignat*}
\end{equation}
\begin{itemize}
	\item \underline{If $\gamma = 0$} \\
	\begin{equation}
	\begin{alignat*}{2}
	h_0(u v) &= h_0(u) + h_0(v) \\
	\implies \exists c \in \mathbb{R} : h_0(u) &= c \log(u)
	\end{alignat*}
	\end{equation}\newline
	\item \underline{If $\gamma \ne 0$}
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u v) &= h_\gamma(v) u^\gamma + h_\gamma(u)
		\end{alignat*}
		\end{equation} And by symmetry,
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u v) &= h_\gamma(u) v^\gamma + h_\gamma(v)
		\end{alignat*}
		\end{equation} Hence,
		\begin{equation}
		\begin{alignat*}{2}
		h_\gamma(u) v^\gamma + h_\gamma(v) &= h_\gamma(v) u^\gamma + h_\gamma(u) \\
		\iff h_\gamma(u) (v^\gamma - 1) &= h_\gamma(v) (u^\gamma - 1) \\
		\implies \exists d : h_\gamma(u) &= d (u^\gamma - 1)
		\end{alignat*}
		\end{equation} where $d$ is a constant, if we take $d = \frac{1}{c \gamma}$, we get $c h_\gamma(u) = \frac{u^\gamma - 1}{\gamma}$
\end{itemize}
To conclude,
\begin{equation}
\begin{alignat*}{2}
\frac{U(x u) - U(x)}{a(x)} \xrightarrow[x \rightarrow + \infty]{} h(u)
\implies h(u) = c h_\gamma(u)
\end{alignat*}
\end{equation}, for some constant $c$, with the auxiliary function $a$ regularly varying with index $\gamma$.


\section{The domain of attraction problem}





